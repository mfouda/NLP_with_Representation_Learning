{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Language Modeling with KenLM\n",
    "\n",
    "We are going to learn how to use KenLM, a toolkit for language modeling.\n",
    "\n",
    "First of all, install KenLM as follows:\n",
    "- Download and unzip: http://kheafield.com/code/kenlm.tar.gz\n",
    "- You need:\n",
    "    - cmake : https://cmake.org/download/ and unzip.\n",
    "      - Do the following:\n",
    "           ```bash\n",
    "           cd cmake\n",
    "           ./bootstrap\n",
    "           make\n",
    "           make install\n",
    "           ```\n",
    "    - Need Boost >= 1.42.0 and bjam\n",
    "        - Ubuntu: `sudo apt-get install libboost-all-dev`\n",
    "        - Mac OS: `brew install boost; brew install bjam`\n",
    "- cd into kenlm folder and compiling using the following commands:\n",
    "    ```bash\n",
    "    mkdir -p build\n",
    "    cd build\n",
    "    cmake ..\n",
    "    make -j 4\n",
    "    ```\n",
    "- Install python KenLM: \n",
    "    ```bash\n",
    "    pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "    ```\n",
    "- Check out KenLM website for more info: http://kheafield.com/code/kenlm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model with KenLM\n",
    "Let's train a bigram language model and 4-gram language model.  \n",
    "First, download the preprocessed Penn Treebank (Wall Street Journal) dataset from here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data.\n",
    "KenLM doesn't support <unk> token so let's remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This removes all occurences of <unk> tokens\n",
    "# sed is a very handy command for quick processing.  \n",
    "# https://www.tutorialspoint.com/sed/sed_overview.htm\n",
    "!sed -e 's/<unk>//g' data/ptb.train.txt > data/ptb.train.nounk.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 842501 types 10001\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:120012 2:6871827456\n",
      "Statistics:\n",
      "1 10001 D1=0.269406 D2=0.72032 D3+=1.15669\n",
      "2 271372 D1=0.717012 D2=1.07895 D3+=1.44702\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 5024 assuming -p 1.5\n",
      "probing 5063 assuming -r models -p 1.5\n",
      "trie    1725 without quantization\n",
      "trie     964 assuming -q 8 -b 8 quantization \n",
      "trie    1725 assuming -a 22 array pointer compression\n",
      "trie     964 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "RSSMax:977702912 kB\tuser:1.44091\tsys:1.93309\tCPU:3.37405\treal:3.79169\n"
     ]
    }
   ],
   "source": [
    "#bigram\n",
    "# !./kenlm/build/bin/lmplz -o 2 < data/ptb.train.nounk.txt > data/ptb_lm_2gram.arpa\n",
    "!<path where you unzipped kenlm>/kenlm/build/bin/lmplz -o 2 < data/ptb.train.nounk.txt > data/ptb_lm_2gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 842501 types 10001\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:120012 2:1169672704 3:2193136384 4:3509018368\n",
      "Statistics:\n",
      "1 10001 D1=0.269406 D2=0.72032 D3+=1.15669\n",
      "2 271372 D1=0.736147 D2=1.10173 D3+=1.46771\n",
      "3 578317 D1=0.878891 D2=1.26107 D3+=1.45765\n",
      "4 685219 D1=0.930799 D2=1.34496 D3+=1.30068\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 32213 assuming -p 1.5\n",
      "probing 37231 assuming -r models -p 1.5\n",
      "trie    14059 without quantization\n",
      "trie     7265 assuming -q 8 -b 8 quantization \n",
      "trie    12759 assuming -a 22 array pointer compression\n",
      "trie     5965 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952 3:11566340 4:16445256\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:120012 2:4341952 3:11566340 4:16445256\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "RSSMax:1381646336 kB\tuser:1.99482\tsys:1.09258\tCPU:3.08746\treal:3.02948\n"
     ]
    }
   ],
   "source": [
    "# 4-gram\n",
    "# !./kenlm/build/bin/lmplz -o 4 < data/ptb.train.nounk.txt > data/ptb_lm_4gram.arpa\n",
    "!<path where you unzipped kenlm>/kenlm/build/bin/lmplz -o 4 < data/ptb.train.nounk.txt > data/ptb_lm_4gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring using KenLM\n",
    "Let's score a sentence using the language model we just trained.  \n",
    "**Note that the score KenLM returns is log likelihood, not perplexity!**  \n",
    "Pereplexity is defined as follow: $$ PPL = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b q(x_i)} $$  \n",
    "\n",
    "All probabilities here are in log base 10 so to convert to perplexity, we do the following:  \n",
    "$$PPL = 10^{-\\log(P) / N} $$\n",
    "where $-\\log(P)$ is the total NLL of the whole sentence, and $N$ is the word count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pre-trained LMs\n",
    "bigram_model = kenlm.LanguageModel('data/ptb_lm_2gram.arpa')\n",
    "fourgram_model = kenlm.LanguageModel('data/ptb_lm_4gram.arpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for calculating perplexity\n",
    "def get_ppl(model, sent):\n",
    "    return 10**(-model.score(sent)/len(sent.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"dividend yields have been bolstered by stock declines \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PPL of a sentence from PTB test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.9773725405043\n",
      "733.1557213309632\n"
     ]
    }
   ],
   "source": [
    "print(get_ppl(bigram_model, sentence))\n",
    "print(get_ppl(fourgram_model, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PPL of an out-of-domain sentence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13349.78268920608\n",
      "13699.961190363858\n"
     ]
    }
   ],
   "source": [
    "ood_sentence = 'artificial neural networks are computing systems vaguely inspired by the biological neural networks'\n",
    "print(get_ppl(bigram_model, ood_sentence))\n",
    "print(get_ppl(fourgram_model, ood_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the perplexity so high?**\n",
    "\n",
    "In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. (from [Wikipedia](https://en.wikipedia.org/wiki/Perplexity))\n",
    "\n",
    "Therefore, it suggests that the model can't predict well for out-of-domain sentences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the sentence above to get novel N-grams (for the same sentence) and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock bolstered declines dividend by yields have been\n",
      "3207.5970808942507\n",
      "3302.2615231292616\n"
     ]
    }
   ],
   "source": [
    "random.seed(555)\n",
    "tmp = sentence.split()\n",
    "random.shuffle(tmp)\n",
    "tmp_sent_2 = ' '.join(tmp)\n",
    "print(tmp_sent_2)\n",
    "print(get_ppl(bigram_model, tmp_sent_2))\n",
    "print(get_ppl(fourgram_model, tmp_sent_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that perplexity gets higher, but not as high as the out-of-domain sentence. \n",
    "\n",
    "**Why?**\n",
    "\n",
    "Shuffle only introduce new n-grams with n>1, while the unigrams remain the same. A shuffled sentence with few out-of-domain words has lower perplexity than oov sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
