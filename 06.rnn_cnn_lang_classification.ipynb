{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Character-based Language Classification \n",
    "\n",
    "In this session we are going to build a character-based model that learns to classify which language does the word belong to.\n",
    "\n",
    "This piece of work will show how to implement Recurrent Neural Net (RNN) and Convolutional Neural Net (CNN) based models and compare them with Bag-of-Words.\n",
    "\n",
    "*Important Note: Make sure to Restart and Run all (Kernel -> Restart and Run all) every time you modify your network before training it: Jupyter Notebook saves network weight and resumes training instead of starting it from scratch again.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0*. Data Generation\n",
    "The dataset (data.p) of roughly 8k words per each language (English, German, French, Bulgarian and Russian) is available in this repo under `data` folder. The following code piece demonstrates how the data is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import random\n",
    "import pickle as pkl\n",
    "from wordfreq import top_n_list\n",
    "from transliterate import translit\n",
    "\n",
    "\n",
    "def select_unique(lang_words, all_unique_words):\n",
    "    unique_words = []\n",
    "    for w in lang_words:\n",
    "        if w in all_unique_words:\n",
    "            unique_words.append(w)\n",
    "    return unique_words\n",
    "\n",
    "def create_splits(word_list, label):\n",
    "    random.shuffle(word_list)\n",
    "    total_len = len(word_list)\n",
    "    train = zip(word_list[0:total_len-2000], [label]*(total_len-2000))\n",
    "    valid = zip(word_list[total_len-2000:total_len-1000] , [label]*(1000))\n",
    "    test = zip(word_list[total_len-1000:], [label]*(1000))\n",
    "    \n",
    "    data_dict= {\"train\": train,\n",
    "               \"valid\": valid,\n",
    "               \"test\": test}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "# Get top 10k words for each language\n",
    "num_words = 10000\n",
    "en_words = top_n_list('en', num_words)\n",
    "de_words = top_n_list('de', num_words)\n",
    "fr_words = top_n_list('fr', num_words)\n",
    "bg_words = top_n_list('bg', num_words)\n",
    "ru_words = top_n_list('ru', num_words)\n",
    "\n",
    "# convert Cyrillic to Latin for Russian language\n",
    "for i in range(len(ru_words)):\n",
    "    ru_words[i] = translit(ru_words[i], 'ru', reversed=True)\n",
    "    \n",
    "for i in range(len(bg_words)):\n",
    "    bg_words[i] = translit(bg_words[i], 'bg', reversed=True)\n",
    "    \n",
    "# Get unique words from all languages\n",
    "all_words = en_words + de_words + fr_words + bg_words + ru_words\n",
    "all_unique_words = set([x for x in all_words if all_words.count(x) == 1])\n",
    "all_unique_words = list(all_unique_words)\n",
    "\n",
    "# Select unique words from each language according to all possible unique words\n",
    "en_unique_words = select_unique(en_words, all_unique_words)\n",
    "de_unique_words = select_unique(de_words, all_unique_words)\n",
    "fr_unique_words = select_unique(fr_words, all_unique_words)\n",
    "bg_unique_words = select_unique(bg_words, all_unique_words)\n",
    "ru_unique_words = select_unique(ru_words, all_unique_words)\n",
    "\n",
    "# Split dataset into train/valid/test\n",
    "data = {\"train\": [], \"valid\": [], \"test\": []}\n",
    "for i, lang in enumerate([en_unique_words, de_unique_words, fr_unique_words, bg_unique_words, ru_unique_words]):\n",
    "    lang_data = create_splits(lang, i)\n",
    "    for key in data.keys():\n",
    "        data[key] += lang_data[key]\n",
    "        \n",
    "# Save\n",
    "pkl.dump(data, open(\"data/data.p\", \"wb\")) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    # Returns:\n",
    "    # id2char: list of chars, where id2char[i] returns char that corresponds to char i\n",
    "    # char2id: dictionary where keys represent chars and corresponding values represent indices\n",
    "    # some preprocessing\n",
    "    max_len = max([len(word[0]) for word in data])\n",
    "    all_chars = []\n",
    "    for word in data:\n",
    "        all_chars += word[0]\n",
    "    unique_chars = list(set(all_chars))\n",
    "\n",
    "    id2char = unique_chars\n",
    "    char2id = dict(zip(unique_chars, range(2,2+len(unique_chars))))\n",
    "    id2char = ['<pad>', '<unk>'] + id2char\n",
    "    char2id['<pad>'] = PAD_IDX\n",
    "    char2id['<unk>'] = UNK_IDX\n",
    "\n",
    "    return char2id, id2char, max_len\n",
    "\n",
    "def convert_to_chars(data):\n",
    "    return [([c for c in sample[0]], sample[1]) for sample in data]\n",
    "\n",
    "### Function that preprocessed dataset\n",
    "def read_data():\n",
    "#     data = pkl.load(open(\"data/rnn_cnn_lang_classification_data.p\", \"rb\"))\n",
    "    data = pkl.load(open(\"data/data.p\", \"rb\"))\n",
    "    train_data, val_data, test_data = data['train'], data['valid'], data['test']\n",
    "    train_data, val_data, test_data = convert_to_chars(train_data), convert_to_chars(val_data), convert_to_chars(test_data)\n",
    "    char2id, id2char, max_len = build_vocab(train_data)\n",
    "    return train_data, val_data, test_data, char2id, id2char, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum word length of dataset is 23\n",
      "Number of characters in dataset is 63\n",
      "Characters:\n",
      "dict_keys(['h', 'â', 'à', 'i', 'x', 'ö', 's', 'ê', 'ü', '►', 'é', 'î', 'e', '6', '.', 'y', '1', '3', 'è', ',', 'u', 'p', 'k', 'ç', 'ï', 'm', 'n', 'b', 'f', 'd', 'q', 'z', 'œ', 'j', 'э', '2', \"'\", '♪', '5', '4', '■', 'ѝ', 'ù', 'ä', 'l', 'ј', 'r', 'v', '0', 'û', 'g', 'ë', '♫', '9', '8', 'c', 'o', 'ô', 'a', 'w', 't', '<pad>', '<unk>'])\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, char2id, id2char, MAX_WORD_LENGTH = read_data()\n",
    "\n",
    "print (\"Maximum word length of dataset is {}\".format(MAX_WORD_LENGTH))\n",
    "print (\"Number of characters in dataset is {}\".format(len(id2char)))\n",
    "print (\"Characters:\")\n",
    "print (char2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the PyTorch DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, char2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        char_idx = [self.char2id[c] if c in self.char2id.keys() else UNK_IDX  for c in self.data_list[key][:MAX_WORD_LENGTH]]\n",
    "        label = self.target_list[key]\n",
    "        return [char_idx, len(char_idx), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    data_list = np.array(data_list)[ind_dec_order]\n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "train_dataset = VocabDataset(train_data, char2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data, char2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_data, char2id)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Net (RNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        # reset hidden state\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(x)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        # forward propagation though RNN\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        ## IMPORTANT ## rnn_out = batch_size * seq_length * hidden_size\n",
    "        \n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out = torch.sum(rnn_out, dim=1)   # HINT: do something along the line of torch.max(0, torch.max(rnn_out, dim=1) or use max_pool1d\n",
    "\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Important things to keep in mind when using variable sized sequences in RNN in Pytorch]**\n",
    "\n",
    "RNN modules accept packed sequences as inputs\n",
    "* pack_padded_sequence function packs a sequence (in Tensor format) containing padded sequences of variable length. **IMPORTANT: the sequences should be sorted by length in a decreasing order before passing to this function**\n",
    "\n",
    "* pad_packed_sequence function is an inverse operation to pack_padded_sequence. Transforms a padded sequence into a tensor of variable lenth sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs = model(data, lengths)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # validate every 256 iterations\n",
    "#         if i > 0 and i % 128 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, total_step, val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN Exercises\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Implement LSTM cell instead of RNN cell. Train the model and compare the results.\n",
    "- Hint (modify init_hidden function and cell in __init__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)  # change from random to zeros?\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        rnn_out, (self.hidden, self.cell) = self.rnn(embed, (self.hidden, self.cell))\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        rnn_out = torch.sum(rnn_out, dim=1)\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [129/924], Validation Acc: 50.02\n",
      "Epoch: [1/10], Step: [257/924], Validation Acc: 57.78\n",
      "Epoch: [1/10], Step: [385/924], Validation Acc: 61.08\n",
      "Epoch: [1/10], Step: [513/924], Validation Acc: 66.2\n",
      "Epoch: [1/10], Step: [641/924], Validation Acc: 69.52\n",
      "Epoch: [1/10], Step: [769/924], Validation Acc: 72.02\n",
      "Epoch: [1/10], Step: [897/924], Validation Acc: 73.32\n",
      "Epoch: [2/10], Step: [129/924], Validation Acc: 73.98\n",
      "Epoch: [2/10], Step: [257/924], Validation Acc: 75.0\n",
      "Epoch: [2/10], Step: [385/924], Validation Acc: 75.98\n",
      "Epoch: [2/10], Step: [513/924], Validation Acc: 77.68\n",
      "Epoch: [2/10], Step: [641/924], Validation Acc: 76.52\n",
      "Epoch: [2/10], Step: [769/924], Validation Acc: 77.92\n",
      "Epoch: [2/10], Step: [897/924], Validation Acc: 79.74\n",
      "Epoch: [3/10], Step: [129/924], Validation Acc: 79.34\n",
      "Epoch: [3/10], Step: [257/924], Validation Acc: 80.4\n",
      "Epoch: [3/10], Step: [385/924], Validation Acc: 80.44\n",
      "Epoch: [3/10], Step: [513/924], Validation Acc: 80.54\n",
      "Epoch: [3/10], Step: [641/924], Validation Acc: 79.4\n",
      "Epoch: [3/10], Step: [769/924], Validation Acc: 81.12\n",
      "Epoch: [3/10], Step: [897/924], Validation Acc: 81.62\n",
      "Epoch: [4/10], Step: [129/924], Validation Acc: 80.96\n",
      "Epoch: [4/10], Step: [257/924], Validation Acc: 81.92\n",
      "Epoch: [4/10], Step: [385/924], Validation Acc: 81.86\n",
      "Epoch: [4/10], Step: [513/924], Validation Acc: 82.0\n",
      "Epoch: [4/10], Step: [641/924], Validation Acc: 82.26\n",
      "Epoch: [4/10], Step: [769/924], Validation Acc: 82.22\n",
      "Epoch: [4/10], Step: [897/924], Validation Acc: 82.3\n",
      "Epoch: [5/10], Step: [129/924], Validation Acc: 82.78\n",
      "Epoch: [5/10], Step: [257/924], Validation Acc: 83.44\n",
      "Epoch: [5/10], Step: [385/924], Validation Acc: 83.38\n",
      "Epoch: [5/10], Step: [513/924], Validation Acc: 83.3\n",
      "Epoch: [5/10], Step: [641/924], Validation Acc: 81.14\n",
      "Epoch: [5/10], Step: [769/924], Validation Acc: 83.12\n",
      "Epoch: [5/10], Step: [897/924], Validation Acc: 83.66\n",
      "Epoch: [6/10], Step: [129/924], Validation Acc: 82.7\n",
      "Epoch: [6/10], Step: [257/924], Validation Acc: 84.08\n",
      "Epoch: [6/10], Step: [385/924], Validation Acc: 83.66\n",
      "Epoch: [6/10], Step: [513/924], Validation Acc: 82.8\n",
      "Epoch: [6/10], Step: [641/924], Validation Acc: 83.94\n",
      "Epoch: [6/10], Step: [769/924], Validation Acc: 83.18\n",
      "Epoch: [6/10], Step: [897/924], Validation Acc: 84.06\n",
      "Epoch: [7/10], Step: [129/924], Validation Acc: 84.32\n",
      "Epoch: [7/10], Step: [257/924], Validation Acc: 82.1\n",
      "Epoch: [7/10], Step: [385/924], Validation Acc: 82.9\n",
      "Epoch: [7/10], Step: [513/924], Validation Acc: 84.5\n",
      "Epoch: [7/10], Step: [641/924], Validation Acc: 84.24\n",
      "Epoch: [7/10], Step: [769/924], Validation Acc: 84.6\n",
      "Epoch: [7/10], Step: [897/924], Validation Acc: 84.74\n",
      "Epoch: [8/10], Step: [129/924], Validation Acc: 84.04\n",
      "Epoch: [8/10], Step: [257/924], Validation Acc: 84.42\n",
      "Epoch: [8/10], Step: [385/924], Validation Acc: 84.4\n",
      "Epoch: [8/10], Step: [513/924], Validation Acc: 84.66\n",
      "Epoch: [8/10], Step: [641/924], Validation Acc: 84.26\n",
      "Epoch: [8/10], Step: [769/924], Validation Acc: 84.44\n",
      "Epoch: [8/10], Step: [897/924], Validation Acc: 84.74\n",
      "Epoch: [9/10], Step: [129/924], Validation Acc: 85.32\n",
      "Epoch: [9/10], Step: [257/924], Validation Acc: 84.74\n",
      "Epoch: [9/10], Step: [385/924], Validation Acc: 84.3\n",
      "Epoch: [9/10], Step: [513/924], Validation Acc: 85.2\n",
      "Epoch: [9/10], Step: [641/924], Validation Acc: 84.7\n",
      "Epoch: [9/10], Step: [769/924], Validation Acc: 84.98\n",
      "Epoch: [9/10], Step: [897/924], Validation Acc: 84.54\n",
      "Epoch: [10/10], Step: [129/924], Validation Acc: 84.8\n",
      "Epoch: [10/10], Step: [257/924], Validation Acc: 84.5\n",
      "Epoch: [10/10], Step: [385/924], Validation Acc: 85.2\n",
      "Epoch: [10/10], Step: [513/924], Validation Acc: 84.88\n",
      "Epoch: [10/10], Step: [641/924], Validation Acc: 84.76\n",
      "Epoch: [10/10], Step: [769/924], Validation Acc: 85.16\n",
      "Epoch: [10/10], Step: [897/924], Validation Acc: 84.4\n"
     ]
    }
   ],
   "source": [
    "model = RNN_LSTM(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TEST LSTM:\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 128 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Implement Bidirectional LSTM. You can do it very easily by adding one argument to cell when you create it.\n",
    "\n",
    "For better understanding we recommend that you implement it youself by reversing a sequence and passing it to another cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_biLSTM_pt(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        super(RNN_biLSTM_pt, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)  # change from random to zeros?\n",
    "        cell = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        rnn_out, (self.hidden, self.cell) = self.rnn(embed, (self.hidden, self.cell))\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        rnn_out = torch.sum(rnn_out, dim=1)\n",
    "        logits = self.linear(rnn_out)\n",
    "#         logits = self.linear(rnn_out[:, -1, :])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_biLSTM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        super(RNN_biLSTM, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.num_layers = self.num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn_forward = nn.LSTM(emb_size, hidden_size, self.num_layers, batch_first=True)\n",
    "        self.rnn_backward = nn.LSTM(emb_size, hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)  # change from random to zeros?\n",
    "        cell = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        self.hidden_back, self.cell_back = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        rnn_out, (self.hidden, self.cell) = self.rnn_forward(embed, (self.hidden, self.cell))\n",
    "        rnn_out_back, (self.hidden_back, self.cell_back) = self.rnn_backward(reversed(embed), (self.hidden_back, self.cell_back))\n",
    "        rnn_out_c = torch.cat((rnn_out, rnn_out_back), 2)\n",
    "        rnn_out_c = torch.sum(rnn_out_c, dim=1)\n",
    "        logits = self.linear(rnn_out_c)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [129/924], Validation Acc: 52.98\n",
      "Epoch: [1/10], Step: [257/924], Validation Acc: 56.96\n",
      "Epoch: [1/10], Step: [385/924], Validation Acc: 59.96\n",
      "Epoch: [1/10], Step: [513/924], Validation Acc: 61.78\n",
      "Epoch: [1/10], Step: [641/924], Validation Acc: 61.96\n",
      "Epoch: [1/10], Step: [769/924], Validation Acc: 65.46\n",
      "Epoch: [1/10], Step: [897/924], Validation Acc: 66.9\n",
      "Epoch: [2/10], Step: [129/924], Validation Acc: 68.34\n",
      "Epoch: [2/10], Step: [257/924], Validation Acc: 68.5\n",
      "Epoch: [2/10], Step: [385/924], Validation Acc: 69.22\n",
      "Epoch: [2/10], Step: [513/924], Validation Acc: 69.94\n",
      "Epoch: [2/10], Step: [641/924], Validation Acc: 70.96\n",
      "Epoch: [2/10], Step: [769/924], Validation Acc: 71.82\n",
      "Epoch: [2/10], Step: [897/924], Validation Acc: 72.28\n",
      "Epoch: [3/10], Step: [129/924], Validation Acc: 72.18\n",
      "Epoch: [3/10], Step: [257/924], Validation Acc: 73.44\n",
      "Epoch: [3/10], Step: [385/924], Validation Acc: 74.46\n",
      "Epoch: [3/10], Step: [513/924], Validation Acc: 74.02\n",
      "Epoch: [3/10], Step: [641/924], Validation Acc: 74.68\n",
      "Epoch: [3/10], Step: [769/924], Validation Acc: 75.34\n",
      "Epoch: [3/10], Step: [897/924], Validation Acc: 76.18\n",
      "Epoch: [4/10], Step: [129/924], Validation Acc: 75.78\n",
      "Epoch: [4/10], Step: [257/924], Validation Acc: 76.26\n",
      "Epoch: [4/10], Step: [385/924], Validation Acc: 76.22\n",
      "Epoch: [4/10], Step: [513/924], Validation Acc: 76.8\n",
      "Epoch: [4/10], Step: [641/924], Validation Acc: 77.26\n",
      "Epoch: [4/10], Step: [769/924], Validation Acc: 77.3\n",
      "Epoch: [4/10], Step: [897/924], Validation Acc: 77.58\n",
      "Epoch: [5/10], Step: [129/924], Validation Acc: 78.38\n",
      "Epoch: [5/10], Step: [257/924], Validation Acc: 78.3\n",
      "Epoch: [5/10], Step: [385/924], Validation Acc: 78.34\n",
      "Epoch: [5/10], Step: [513/924], Validation Acc: 78.52\n",
      "Epoch: [5/10], Step: [641/924], Validation Acc: 79.58\n",
      "Epoch: [5/10], Step: [769/924], Validation Acc: 78.86\n",
      "Epoch: [5/10], Step: [897/924], Validation Acc: 79.74\n",
      "Epoch: [6/10], Step: [129/924], Validation Acc: 79.94\n",
      "Epoch: [6/10], Step: [257/924], Validation Acc: 79.14\n",
      "Epoch: [6/10], Step: [385/924], Validation Acc: 80.06\n",
      "Epoch: [6/10], Step: [513/924], Validation Acc: 80.06\n",
      "Epoch: [6/10], Step: [641/924], Validation Acc: 80.18\n",
      "Epoch: [6/10], Step: [769/924], Validation Acc: 80.82\n",
      "Epoch: [6/10], Step: [897/924], Validation Acc: 79.58\n",
      "Epoch: [7/10], Step: [129/924], Validation Acc: 81.52\n",
      "Epoch: [7/10], Step: [257/924], Validation Acc: 81.32\n",
      "Epoch: [7/10], Step: [385/924], Validation Acc: 80.88\n",
      "Epoch: [7/10], Step: [513/924], Validation Acc: 80.68\n",
      "Epoch: [7/10], Step: [641/924], Validation Acc: 81.22\n",
      "Epoch: [7/10], Step: [769/924], Validation Acc: 81.1\n",
      "Epoch: [7/10], Step: [897/924], Validation Acc: 80.92\n",
      "Epoch: [8/10], Step: [129/924], Validation Acc: 81.68\n",
      "Epoch: [8/10], Step: [257/924], Validation Acc: 80.88\n",
      "Epoch: [8/10], Step: [385/924], Validation Acc: 81.84\n",
      "Epoch: [8/10], Step: [513/924], Validation Acc: 81.12\n",
      "Epoch: [8/10], Step: [641/924], Validation Acc: 81.54\n",
      "Epoch: [8/10], Step: [769/924], Validation Acc: 82.22\n",
      "Epoch: [8/10], Step: [897/924], Validation Acc: 82.18\n",
      "Epoch: [9/10], Step: [129/924], Validation Acc: 82.18\n",
      "Epoch: [9/10], Step: [257/924], Validation Acc: 81.9\n",
      "Epoch: [9/10], Step: [385/924], Validation Acc: 82.74\n",
      "Epoch: [9/10], Step: [513/924], Validation Acc: 82.78\n",
      "Epoch: [9/10], Step: [641/924], Validation Acc: 83.3\n",
      "Epoch: [9/10], Step: [769/924], Validation Acc: 82.66\n",
      "Epoch: [9/10], Step: [897/924], Validation Acc: 82.38\n",
      "Epoch: [10/10], Step: [129/924], Validation Acc: 82.68\n",
      "Epoch: [10/10], Step: [257/924], Validation Acc: 82.84\n",
      "Epoch: [10/10], Step: [385/924], Validation Acc: 83.02\n",
      "Epoch: [10/10], Step: [513/924], Validation Acc: 82.64\n",
      "Epoch: [10/10], Step: [641/924], Validation Acc: 83.32\n",
      "Epoch: [10/10], Step: [769/924], Validation Acc: 83.68\n",
      "Epoch: [10/10], Step: [897/924], Validation Acc: 83.74\n"
     ]
    }
   ],
   "source": [
    "# TEST biLSTM using pytorch:\n",
    "model = RNN_biLSTM_pt(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 128 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [129/924], Validation Acc: 52.98\n",
      "Epoch: [1/10], Step: [257/924], Validation Acc: 57.44\n",
      "Epoch: [1/10], Step: [385/924], Validation Acc: 61.06\n",
      "Epoch: [1/10], Step: [513/924], Validation Acc: 64.94\n",
      "Epoch: [1/10], Step: [641/924], Validation Acc: 67.14\n",
      "Epoch: [1/10], Step: [769/924], Validation Acc: 69.2\n",
      "Epoch: [1/10], Step: [897/924], Validation Acc: 69.28\n",
      "Epoch: [2/10], Step: [129/924], Validation Acc: 71.28\n",
      "Epoch: [2/10], Step: [257/924], Validation Acc: 71.92\n",
      "Epoch: [2/10], Step: [385/924], Validation Acc: 73.58\n",
      "Epoch: [2/10], Step: [513/924], Validation Acc: 74.38\n",
      "Epoch: [2/10], Step: [641/924], Validation Acc: 74.66\n",
      "Epoch: [2/10], Step: [769/924], Validation Acc: 75.02\n",
      "Epoch: [2/10], Step: [897/924], Validation Acc: 75.58\n",
      "Epoch: [3/10], Step: [129/924], Validation Acc: 75.48\n",
      "Epoch: [3/10], Step: [257/924], Validation Acc: 76.62\n",
      "Epoch: [3/10], Step: [385/924], Validation Acc: 75.94\n",
      "Epoch: [3/10], Step: [513/924], Validation Acc: 77.44\n",
      "Epoch: [3/10], Step: [641/924], Validation Acc: 77.6\n",
      "Epoch: [3/10], Step: [769/924], Validation Acc: 78.44\n",
      "Epoch: [3/10], Step: [897/924], Validation Acc: 78.06\n",
      "Epoch: [4/10], Step: [129/924], Validation Acc: 79.08\n",
      "Epoch: [4/10], Step: [257/924], Validation Acc: 79.5\n",
      "Epoch: [4/10], Step: [385/924], Validation Acc: 78.54\n",
      "Epoch: [4/10], Step: [513/924], Validation Acc: 79.36\n",
      "Epoch: [4/10], Step: [641/924], Validation Acc: 79.7\n",
      "Epoch: [4/10], Step: [769/924], Validation Acc: 79.76\n",
      "Epoch: [4/10], Step: [897/924], Validation Acc: 79.94\n",
      "Epoch: [5/10], Step: [129/924], Validation Acc: 80.7\n",
      "Epoch: [5/10], Step: [257/924], Validation Acc: 79.98\n",
      "Epoch: [5/10], Step: [385/924], Validation Acc: 80.44\n",
      "Epoch: [5/10], Step: [513/924], Validation Acc: 80.92\n",
      "Epoch: [5/10], Step: [641/924], Validation Acc: 81.36\n",
      "Epoch: [5/10], Step: [769/924], Validation Acc: 80.92\n",
      "Epoch: [5/10], Step: [897/924], Validation Acc: 82.18\n",
      "Epoch: [6/10], Step: [129/924], Validation Acc: 81.92\n",
      "Epoch: [6/10], Step: [257/924], Validation Acc: 81.38\n",
      "Epoch: [6/10], Step: [385/924], Validation Acc: 81.82\n",
      "Epoch: [6/10], Step: [513/924], Validation Acc: 80.78\n",
      "Epoch: [6/10], Step: [641/924], Validation Acc: 79.92\n",
      "Epoch: [6/10], Step: [769/924], Validation Acc: 81.26\n",
      "Epoch: [6/10], Step: [897/924], Validation Acc: 81.4\n",
      "Epoch: [7/10], Step: [129/924], Validation Acc: 82.04\n",
      "Epoch: [7/10], Step: [257/924], Validation Acc: 81.66\n",
      "Epoch: [7/10], Step: [385/924], Validation Acc: 82.02\n",
      "Epoch: [7/10], Step: [513/924], Validation Acc: 82.22\n",
      "Epoch: [7/10], Step: [641/924], Validation Acc: 82.44\n",
      "Epoch: [7/10], Step: [769/924], Validation Acc: 82.44\n",
      "Epoch: [7/10], Step: [897/924], Validation Acc: 82.46\n",
      "Epoch: [8/10], Step: [129/924], Validation Acc: 81.68\n",
      "Epoch: [8/10], Step: [257/924], Validation Acc: 82.4\n",
      "Epoch: [8/10], Step: [385/924], Validation Acc: 82.04\n",
      "Epoch: [8/10], Step: [513/924], Validation Acc: 82.5\n",
      "Epoch: [8/10], Step: [641/924], Validation Acc: 83.04\n",
      "Epoch: [8/10], Step: [769/924], Validation Acc: 82.56\n",
      "Epoch: [8/10], Step: [897/924], Validation Acc: 82.56\n",
      "Epoch: [9/10], Step: [129/924], Validation Acc: 82.14\n",
      "Epoch: [9/10], Step: [257/924], Validation Acc: 80.98\n",
      "Epoch: [9/10], Step: [385/924], Validation Acc: 82.38\n",
      "Epoch: [9/10], Step: [513/924], Validation Acc: 82.82\n",
      "Epoch: [9/10], Step: [641/924], Validation Acc: 83.14\n",
      "Epoch: [9/10], Step: [769/924], Validation Acc: 82.96\n",
      "Epoch: [9/10], Step: [897/924], Validation Acc: 83.24\n",
      "Epoch: [10/10], Step: [129/924], Validation Acc: 83.04\n",
      "Epoch: [10/10], Step: [257/924], Validation Acc: 83.06\n",
      "Epoch: [10/10], Step: [385/924], Validation Acc: 83.16\n",
      "Epoch: [10/10], Step: [513/924], Validation Acc: 82.42\n",
      "Epoch: [10/10], Step: [641/924], Validation Acc: 83.36\n",
      "Epoch: [10/10], Step: [769/924], Validation Acc: 83.2\n",
      "Epoch: [10/10], Step: [897/924], Validation Acc: 83.44\n"
     ]
    }
   ],
   "source": [
    "# TEST biLSTM self-implemented:\n",
    "model = RNN_biLSTM(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 128 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Add max-pooling (over time) after passing through RNN instead of summing over hidden layers through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_maxpool(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        super(RNN_maxpool, self).__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "#         rnn_out = torch.sum(rnn_out, dim=1)\n",
    "        mp = nn.MaxPool2d((lengths.max(),1))\n",
    "        softmax = mp(rnn_out).reshape(batch_size, self.hidden_size)\n",
    "        logits = self.linear(softmax)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [257/924], Validation Acc: 57.98\n",
      "Epoch: [1/10], Step: [513/924], Validation Acc: 64.32\n",
      "Epoch: [1/10], Step: [769/924], Validation Acc: 68.78\n",
      "Epoch: [2/10], Step: [257/924], Validation Acc: 73.1\n",
      "Epoch: [2/10], Step: [513/924], Validation Acc: 75.02\n",
      "Epoch: [2/10], Step: [769/924], Validation Acc: 76.56\n",
      "Epoch: [3/10], Step: [257/924], Validation Acc: 77.52\n",
      "Epoch: [3/10], Step: [513/924], Validation Acc: 77.3\n",
      "Epoch: [3/10], Step: [769/924], Validation Acc: 78.98\n",
      "Epoch: [4/10], Step: [257/924], Validation Acc: 78.74\n",
      "Epoch: [4/10], Step: [513/924], Validation Acc: 79.7\n",
      "Epoch: [4/10], Step: [769/924], Validation Acc: 80.04\n",
      "Epoch: [5/10], Step: [257/924], Validation Acc: 80.2\n",
      "Epoch: [5/10], Step: [513/924], Validation Acc: 81.0\n",
      "Epoch: [5/10], Step: [769/924], Validation Acc: 81.22\n",
      "Epoch: [6/10], Step: [257/924], Validation Acc: 82.06\n",
      "Epoch: [6/10], Step: [513/924], Validation Acc: 81.28\n",
      "Epoch: [6/10], Step: [769/924], Validation Acc: 82.0\n",
      "Epoch: [7/10], Step: [257/924], Validation Acc: 81.94\n",
      "Epoch: [7/10], Step: [513/924], Validation Acc: 82.46\n",
      "Epoch: [7/10], Step: [769/924], Validation Acc: 82.52\n",
      "Epoch: [8/10], Step: [257/924], Validation Acc: 82.62\n",
      "Epoch: [8/10], Step: [513/924], Validation Acc: 83.64\n",
      "Epoch: [8/10], Step: [769/924], Validation Acc: 83.0\n",
      "Epoch: [9/10], Step: [257/924], Validation Acc: 83.26\n",
      "Epoch: [9/10], Step: [513/924], Validation Acc: 82.8\n",
      "Epoch: [9/10], Step: [769/924], Validation Acc: 83.34\n",
      "Epoch: [10/10], Step: [257/924], Validation Acc: 84.12\n",
      "Epoch: [10/10], Step: [513/924], Validation Acc: 83.3\n",
      "Epoch: [10/10], Step: [769/924], Validation Acc: 84.12\n"
     ]
    }
   ],
   "source": [
    "# TEST RNN adding maxpooling:\n",
    "model = RNN_maxpool(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 256 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional Neural Net (CNN) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [129/924], Validation Acc: 64.7\n",
      "Epoch: [1/10], Step: [257/924], Validation Acc: 71.58\n",
      "Epoch: [1/10], Step: [385/924], Validation Acc: 72.6\n",
      "Epoch: [1/10], Step: [513/924], Validation Acc: 77.02\n",
      "Epoch: [1/10], Step: [641/924], Validation Acc: 75.84\n",
      "Epoch: [1/10], Step: [769/924], Validation Acc: 75.04\n",
      "Epoch: [1/10], Step: [897/924], Validation Acc: 76.62\n",
      "Epoch: [2/10], Step: [129/924], Validation Acc: 80.14\n",
      "Epoch: [2/10], Step: [257/924], Validation Acc: 78.42\n",
      "Epoch: [2/10], Step: [385/924], Validation Acc: 81.0\n",
      "Epoch: [2/10], Step: [513/924], Validation Acc: 81.16\n",
      "Epoch: [2/10], Step: [641/924], Validation Acc: 81.92\n",
      "Epoch: [2/10], Step: [769/924], Validation Acc: 82.4\n",
      "Epoch: [2/10], Step: [897/924], Validation Acc: 82.38\n",
      "Epoch: [3/10], Step: [129/924], Validation Acc: 82.32\n",
      "Epoch: [3/10], Step: [257/924], Validation Acc: 81.7\n",
      "Epoch: [3/10], Step: [385/924], Validation Acc: 81.64\n",
      "Epoch: [3/10], Step: [513/924], Validation Acc: 81.2\n",
      "Epoch: [3/10], Step: [641/924], Validation Acc: 83.04\n",
      "Epoch: [3/10], Step: [769/924], Validation Acc: 82.8\n",
      "Epoch: [3/10], Step: [897/924], Validation Acc: 81.76\n",
      "Epoch: [4/10], Step: [129/924], Validation Acc: 83.58\n",
      "Epoch: [4/10], Step: [257/924], Validation Acc: 82.82\n",
      "Epoch: [4/10], Step: [385/924], Validation Acc: 83.64\n",
      "Epoch: [4/10], Step: [513/924], Validation Acc: 82.58\n",
      "Epoch: [4/10], Step: [641/924], Validation Acc: 83.28\n",
      "Epoch: [4/10], Step: [769/924], Validation Acc: 84.62\n",
      "Epoch: [4/10], Step: [897/924], Validation Acc: 84.48\n",
      "Epoch: [5/10], Step: [129/924], Validation Acc: 83.5\n",
      "Epoch: [5/10], Step: [257/924], Validation Acc: 84.44\n",
      "Epoch: [5/10], Step: [385/924], Validation Acc: 84.66\n",
      "Epoch: [5/10], Step: [513/924], Validation Acc: 84.58\n",
      "Epoch: [5/10], Step: [641/924], Validation Acc: 82.82\n",
      "Epoch: [5/10], Step: [769/924], Validation Acc: 84.6\n",
      "Epoch: [5/10], Step: [897/924], Validation Acc: 83.92\n",
      "Epoch: [6/10], Step: [129/924], Validation Acc: 84.34\n",
      "Epoch: [6/10], Step: [257/924], Validation Acc: 84.64\n",
      "Epoch: [6/10], Step: [385/924], Validation Acc: 82.86\n",
      "Epoch: [6/10], Step: [513/924], Validation Acc: 84.0\n",
      "Epoch: [6/10], Step: [641/924], Validation Acc: 84.9\n",
      "Epoch: [6/10], Step: [769/924], Validation Acc: 84.04\n",
      "Epoch: [6/10], Step: [897/924], Validation Acc: 83.94\n",
      "Epoch: [7/10], Step: [129/924], Validation Acc: 85.14\n",
      "Epoch: [7/10], Step: [257/924], Validation Acc: 83.5\n",
      "Epoch: [7/10], Step: [385/924], Validation Acc: 84.04\n",
      "Epoch: [7/10], Step: [513/924], Validation Acc: 84.5\n",
      "Epoch: [7/10], Step: [641/924], Validation Acc: 84.36\n",
      "Epoch: [7/10], Step: [769/924], Validation Acc: 83.04\n",
      "Epoch: [7/10], Step: [897/924], Validation Acc: 84.88\n",
      "Epoch: [8/10], Step: [129/924], Validation Acc: 85.3\n",
      "Epoch: [8/10], Step: [257/924], Validation Acc: 85.06\n",
      "Epoch: [8/10], Step: [385/924], Validation Acc: 85.26\n",
      "Epoch: [8/10], Step: [513/924], Validation Acc: 85.18\n",
      "Epoch: [8/10], Step: [641/924], Validation Acc: 85.56\n",
      "Epoch: [8/10], Step: [769/924], Validation Acc: 84.82\n",
      "Epoch: [8/10], Step: [897/924], Validation Acc: 85.86\n",
      "Epoch: [9/10], Step: [129/924], Validation Acc: 85.26\n",
      "Epoch: [9/10], Step: [257/924], Validation Acc: 85.22\n",
      "Epoch: [9/10], Step: [385/924], Validation Acc: 84.96\n",
      "Epoch: [9/10], Step: [513/924], Validation Acc: 85.46\n",
      "Epoch: [9/10], Step: [641/924], Validation Acc: 85.4\n",
      "Epoch: [9/10], Step: [769/924], Validation Acc: 85.16\n",
      "Epoch: [9/10], Step: [897/924], Validation Acc: 86.02\n",
      "Epoch: [10/10], Step: [129/924], Validation Acc: 85.86\n",
      "Epoch: [10/10], Step: [257/924], Validation Acc: 85.84\n",
      "Epoch: [10/10], Step: [385/924], Validation Acc: 84.74\n",
      "Epoch: [10/10], Step: [513/924], Validation Acc: 85.58\n",
      "Epoch: [10/10], Step: [641/924], Validation Acc: 85.7\n",
      "Epoch: [10/10], Step: [769/924], Validation Acc: 86.12\n",
      "Epoch: [10/10], Step: [897/924], Validation Acc: 85.06\n"
     ]
    }
   ],
   "source": [
    "model = CNN(emb_size=100, hidden_size=200, num_layers=2, num_classes=5, vocab_size=len(id2char))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 128 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, total_step, val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using Convolutional Nets for Language Tasks in Pytorch\n",
    "\n",
    "### Conv1d module expect input of size (batch_size, num_channels, length), where in our case input has size (batch_size, length, num_channels). Hence it is important call transpose(1,2) before passing it to convolutional layer and then reshape it back to (batch_size, length, num_channels) by calling transpose(1,2) again\n",
    "\n",
    "### Additionally we need to reshape hidden activations into 2D tensor before passing it to Relu layer by calling view(-1, hidden.size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "### Implement Gated Relu activations as well as Gated Linear activations and compare them with Relu (reference: https://arxiv.org/pdf/1612.08083.pdf )\n",
    "### Hint: Gated Relu activations are sigmoid(conv1_1(x)) * relu(conv1_2(x))\n",
    "### Hint: Gated Linear activations are sigmoid(conv1_1(x)) * conv1_2(x)\n",
    "\n",
    "### Feel free to play with other variants of gating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:\n",
    "\n",
    "### Add max-pooling (over time) after passing through conv as well as add non-linear fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6:\n",
    "\n",
    "### Use Bag-of-Words and Bag-of-NGrams model for this task and compare it with RNN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7:\n",
    "\n",
    "### Use FastText for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
