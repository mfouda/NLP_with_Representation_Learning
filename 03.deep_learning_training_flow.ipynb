{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Deep Learning Training Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a simple Logistic Regression model using a standard deep learning training workflow in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Digits dataset (scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a digits data set. Each $x$ is an 8x8 matrix representing a hand-written digits, and the $y$ is which of the 10 digits it represented.\n",
    "\n",
    "**Note**: This is *not* MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "raw_data = datasets.load_digits()\n",
    "print(raw_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.data.shape)\n",
    "print(raw_data.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a217a66d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(raw_data.data[0].reshape(8, 8), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to split our data into train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(raw_data.data, raw_data.target, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = \\\n",
    "    train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1149, 64) (288, 64) (360, 64)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed - two things about deep learning training workflows.\n",
    "\n",
    "Unlike in the case of regressions (as before), we often cannot fit all the data into memory--particularly when training on GPUs, which often have less memory. Hence, we often train the models iteratively in **batches** (see: *minibatch gradient descent*).\n",
    "\n",
    "Because we do gradient descent, we often also go over the data multiple times--in multiple **epochs**. We need to specify how many epochs to train for (later, you will learn other ways to step epochs early, or potentially not use epochs at all).\n",
    "\n",
    "Here, we can easily fit all the data into memory, but we will pretend we cannot, and set our batch-size per gradient descent step to 32--so we're training on 32 instances per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are doing to subclass PyTorch's `Dataset` class. A Dataset class can be used to represent any kind of data. Importantly, you need to implement `__getitem__` and `__len__` methods. \n",
    "\n",
    "`__getitem__` in particular has a fixed signature, where given a numerical index, it returns the corresponding data for that instance. \n",
    "\n",
    "That is all you need to do to define the Dataset. PyTorch handles the rest in terms of converting to Tensors and batching - in `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        self.length = len(x)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a data set for our train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, y_train)\n",
    "val_dataset = MyDataset(x_val, y_val)\n",
    "test_dataset = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a DataLoader for each data set. Note that we often want to shuffle our training data when we iterate over it, but not necessarily the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy our model from the `logistic_regression` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        logit = self.linear(x)\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create 3 functions here:\n",
    "\n",
    "1. A training method\n",
    "2. An evaluation method\n",
    "3. A method for computing accuracy\n",
    "\n",
    "In both `do_train` and `do_eval`, we iterate over our provided DataLoader, and carry out the forward pass. Note that `x` and `y` are already neatly batched into the correct batch size and converted to Tensors.\n",
    "\n",
    "Note that `do_train` and `do_eval` do have some overlap--but are also quite different. (See if you can spot all the differences.) Most importantly, we need to perform backpropagation in `do_train`, and in `do_eval` we want to record the outputs. It is possible to combine the two, but the function can get quite ugly--this is up to your personal taste.\n",
    "\n",
    "*Make sure you understand every line of these methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x.float())\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accurately compute loss, because of different batch size\n",
    "        loss_val += loss.item() * len(x) / len(dataloader.dataset)\n",
    "    return loss_val\n",
    "\n",
    "def do_eval(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    y_ls = []\n",
    "    y_hat_ls = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y_hat = model(x.float())\n",
    "            loss = criterion(y_hat, y)\n",
    "            # Accurately compute loss, because of different batch size\n",
    "            loss_val += loss.item() * len(x) / len(dataloader.dataset)\n",
    "            y_hat_ls.append(y_hat)\n",
    "            y_ls.append(y)\n",
    "    optimizer.zero_grad()\n",
    "    return loss_val, torch.cat(y_hat_ls, dim=0), torch.cat(y_ls, dim=0)\n",
    "\n",
    "def acc(model, dataloader, criterion):\n",
    "    _, pred, true = do_eval(\n",
    "        model=model, \n",
    "        dataloader=dataloader,\n",
    "        criterion=criterion,\n",
    "    )\n",
    "    return (torch.exp(pred).max(1)[1] == true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model, criterion and optimizer. We also want to record our training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "model = LogisticRegressionPyTorch(\n",
    "    n_in=raw_data.data.shape[1], \n",
    "    n_out=len(raw_data.target_names),\n",
    ")\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the core of our training - we train, and also compute validation loss at each epoch.\n",
    "\n",
    "Note: In some code bases you will often see the core training loop have all sorts of logic here (e.g. batching, data conversion, loss computation, logging, etc). I recommend you refactor those to separate functions/methods, and keep your core loop as clean as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiments & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 50.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pro-tip: Make your core loop CLEAN\n",
    "for epoch in tqdm.trange(N_EPOCHS):\n",
    "    train_loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_pred, val_true = do_eval(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=val_dataloader,\n",
    "    )\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some inspections and plot our training and validation loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a219a1668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history_df = pd.DataFrame({\n",
    "    \"train\": train_loss_history,\n",
    "    \"val\": val_loss_history,\n",
    "})\n",
    "loss_history_df.plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the training, validation and test accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(model, train_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9513888955116272"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(model, val_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611111283302307"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(my in-class note)\n",
    "\n",
    "### For training on GPU:\n",
    "1. both model and training data need to be on GPU:\n",
    "    `x, y = x.to(device), y.to(device)`\n",
    "2. the computed y_hat is on GPU, if you need to to further computation on cpu, fetch the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
