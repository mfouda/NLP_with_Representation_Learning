{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Logistic Regression in scikit-learn and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to implement a simple multi-class logistic regression in both scikit-learn and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data: Iris Dataset (scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Iris data set from scikit-learn - a 150 instance, 3-class data set with 4 features.\n",
    "\n",
    "For now, we won't bother with the standard train/test splits and simply build a working model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our target $y$ is encoded as a single array with classes as its values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in `LogisticRegression` classifier in scikit-learn. To make this implementation consistent with our PyTorch formulation, we will slightly modify the defaults - namely modifying the multi-class calculation to use a softmax, and turning off the regularization.\n",
    "\n",
    "(If you do not know what 'regularization' means, it will be covered in a later lecture. For now, just know it is an additional part of the model we are not concerned with.)\n",
    "\n",
    "First, we set up the model with our desired arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sk_model = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000000,\n",
    "    C=np.finfo(np.float).max,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.7976931348623157e+308, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=1000000,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.fit(X=iris.data, y=iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can predict probabilities that each sample is in each class. Note that the probabilities here will be *very* high, because we are clearly overfitting to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 9.71822246e-35, 1.49417055e-61],\n",
       "       [1.00000000e+00, 1.13758507e-27, 8.08705362e-53],\n",
       "       [1.00000000e+00, 5.53256279e-31, 6.59096956e-57],\n",
       "       [1.00000000e+00, 1.08570929e-25, 2.12830090e-50],\n",
       "       [1.00000000e+00, 1.70014979e-35, 1.71464960e-62]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.predict_proba(X=iris.data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import some modules from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a model in PyTorch -- as an `nn.Module`.\n",
    "\n",
    "A `nn.Module` can really be any function, but it is often used to implement layers, functions and models. Note that you can also nest modules.\n",
    "\n",
    "Importantly, modules need to have their `forward()` method overridden, and very often you will want to override the `__init__` method as well. \n",
    "\n",
    "The `__init__` method sets up the module, akin to how we set up the `LogisticRegression` model above with some arguments. This is also often where the internal modules and parameters are initialized.\n",
    "\n",
    "The `forward` method defines what happens when you *apply* the module.\n",
    "\n",
    "In the background, PyTorch makes use of your code in the forward method and determines how to implement back-propagation with it - but all you need to do is to define the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(x)\n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(iris.data).float()\n",
    "y = torch.from_numpy(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List to record our loss over time\n",
    "loss_val_ls = []\n",
    "\n",
    "# Initialize our model. Note we need to provide n_in and n_out\n",
    "pt_model = LogisticRegressionPyTorch(n_in=x.shape[1], n_out=3)\n",
    "\n",
    "# Set up the criterion - loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Set up the optimizer. We need to tell the optimizer which\n",
    "#   parameters it will optimize over (which parameters it is\n",
    "#   allowed to modify).\n",
    "optimizer = optim.Adam(pt_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# As a quick check, lets investigate the number of parameters in our model:\n",
    "for param in pt_model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run over many iterations!\n",
    "for i in range(10000):\n",
    "    \n",
    "    # Always zero-out the gradients managed by your optimizer\n",
    "    # PyTorch does not automatically zero-out your gradients\n",
    "    #   You can also do pt_model.zero_grad() in this case.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # !! Put model into training mode. This does not do anything\n",
    "    #   in a simple Logistic Regression model, but will be important \n",
    "    #   later. (See: Dropout)\n",
    "    pt_model.train()\n",
    "    \n",
    "    # Compute the predicted log-probabilities\n",
    "    y_hat = pt_model(x)\n",
    "    \n",
    "    # Compute the loss\n",
    "    train_loss = criterion(y_hat, y)\n",
    "    \n",
    "    # Back-propagate the gradients to the parameters\n",
    "    train_loss.backward()\n",
    "    \n",
    "    # Apply the gradient updates to the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Recompute the loss in evaluation mode, and record it.\n",
    "    # Again, this does not do anything here, but will be important later.\n",
    "    # Since we are evaluating, we will also tell PyTorch not to\n",
    "    #   compute gradients.\n",
    "    pt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = pt_model(x)\n",
    "        eval_loss = criterion(y_hat, y)\n",
    "        \n",
    "    # Record the loss\n",
    "    # Note that 'loss' is a Tensor, but loss.item() is a number\n",
    "    loss_val_ls.append(eval_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Log Loss')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGkhJREFUeJzt3X2UZHV95/H3p7p7uoeZnudGxpmBHhRzRFd56EWIOS4n6yqQLGQjqzMrimjCCYkHje6DmLMm69k9J+bsEpfFFQkPBh/QKEpGDgZdJGtYldAgTwNOGJ5HEHqGh3mie7q7vvvH/XVN9UP1rZnp29Xd9/M65566de+vbn1v3er69H1WRGBmZgZQaXUBZmY2dzgUzMysxqFgZmY1DgUzM6txKJiZWY1DwczMahwKZmZW41AwM7Mah4KZmdW0t7qAQ7VmzZro7e1tdRlmZvPKPffcszMievLazbtQ6O3tpb+/v9VlmJnNK5KeaqadNx+ZmVmNQ8HMzGocCmZmVuNQMDOzGoeCmZnVOBTMzKzGoWBmZjWlCYV/en4Pl/9gGzv3DrW6FDOzOas0ofDo83u54kfbeXHfgVaXYmY2Z5UmFMzMLJ9DwczMakoXChGtrsDMbO4qTShIra7AzGzuK00omJlZPoeCmZnVOBTMzKymdKEQeE+zmVkjpQkF72c2M8tXmlAwM7N8DgUzM6spLBQkbZB0h6RHJG2V9LEp2pwp6RVJ96XuM0XVM8Ynr5mZNdZe4LRHgE9GxL2SuoF7JP0wIh6e0O4fIuK3C6wD8MlrZmbNKGxNISKei4h7U/8e4BFgXVHvZ2ZmR25W9ilI6gVOBu6aYvQZku6X9H1Jb5qNeszMbGpFbj4CQNJS4Cbg4xGxe8Loe4HjImKvpHOAm4ETppjGxcDFAMcee2zBFZuZlVehawqSOsgC4WsR8Z2J4yNid0TsTf23Ah2S1kzR7uqI6IuIvp6eniOqyTuazcwaK/LoIwHXAo9ExOUN2hyT2iHptFTProIqKmayZmYLSJGbj94OfAB4UNJ9adingWMBIuIq4HzgEkkjwKvApgj/L29m1iqFhUJE3EnOv+cRcSVwZVE1mJnZoSndGc2+IJ6ZWWOlCQWfvGZmlq80oWBmZvkcCmZmVlO6UPCxTWZmjZUmFLxLwcwsX2lCwczM8jkUzMysxqFgZmY1DgUzM6spTSjIZ6+ZmeUqTSiYmVk+h4KZmdWULhR88pqZWWOlCQXvUTAzy1eaUDAzs3wOBTMzqyldKPgmO2ZmjZUmFHyagplZvtKEgpmZ5XMomJlZjUPBzMxqShcKPnnNzKyx0oSCdzSbmeUrTSiYmVk+h4KZmdWULhS8S8HMrLHShIJ8STwzs1yFhYKkDZLukPSIpK2SPjZFG0m6QtJ2SQ9IOqWoeszMLF97gdMeAT4ZEfdK6gbukfTDiHi4rs3ZwAmpexvwxfRoZmYtUNiaQkQ8FxH3pv49wCPAugnNzgNuiMzPgBWS1hZVk5mZTW9W9ilI6gVOBu6aMGod8Ezd8x1MDo4ZFT57zcysocJDQdJS4Cbg4xGxe+LoKV4y6Vdb0sWS+iX1DwwMHGYhh/cyM7MyKTQUJHWQBcLXIuI7UzTZAWyoe74eeHZio4i4OiL6IqKvp6enmGLNzKzQo48EXAs8EhGXN2i2BfhgOgrpdOCViHiuqJrMzGx6RR599HbgA8CDku5Lwz4NHAsQEVcBtwLnANuB/cBFBdYD+OQ1M7PpFBYKEXEnOVvyI9vr+0dF1VDPuxTMzPKV5oxmMzPL51AwM7Oa0oWCT1MwM2usNKEg32XHzCxXaULBzMzyORTMzKzGoWBmZjUlDAXvaTYza6Q0oeDdzGZm+UoTCmZmls+hYGZmNaULBZ+8ZmbWWGlCweeumZnlK00omJlZPoeCmZnVlC4UvEvBzKyx0oSCfKaCmVmu0oSCmZnlcyiYmVmNQ8HMzGpKFwo+ec3MrLHcUJD0Okmdqf9MSZdKWlF8aTPLJ6+ZmeVrZk3hJmBU0uuBa4GNwNcLrcrMzFqimVCoRsQI8G+Az0fEHwNriy3LzMxaoZlQGJa0GbgQuCUN6yiupGKFdyqYmTXUTChcBJwB/LeIeELSRuCrxZY187xLwcwsX3teg4h4GLgUQNJKoDsi/rzowszMbPY1c/TR30taJmkVcD9wvaTLiy/NzMxmWzObj5ZHxG7gd4HrI+JU4J15L5J0naQXJD3UYPyZkl6RdF/qPnNopZuZ2UxrJhTaJa0F3svBHc3N+DJwVk6bf4iIk1L32UOY9mHzbmYzs8aaCYXPArcBj0XE3ZKOBx7Ne1FE/Bh48Qjrmzne02xmlquZHc3fAr5V9/xx4D0z9P5nSLofeBb49xGxdYama2Zmh6GZHc3rJX037R94XtJNktbPwHvfCxwXEW8F/hdw8zQ1XCypX1L/wMDADLy1mZlNpZnNR9cDW4DXAuuA76VhRyQidkfE3tR/K9AhaU2DtldHRF9E9PX09Bzh+x7Ry83MFrRmQqEnIq6PiJHUfRk4sl9mQNIxUnaZOkmnpVp2Hel0G76fdyqYmeXK3acA7JR0AXBjer6ZJn68Jd0InAmskbQD+FPS5TEi4irgfOASSSPAq8CmKPAaFJGOO9ozOFzUW5iZzXvNhMKHgSuBvyQ7ovMnZJe+mFZEbM4Zf2Wa7qy489GdAPzZlq28603HzNbbmpnNK7mbjyLi6Yg4NyJ6IuLoiPgdshPZ5pX9B0YB2Ds00uJKzMzmrsO989onZrSKWTBazTYftVW8b8HMrJHDDYV598s6GmOhULo7kJqZNe1wfyHn3YGdo6NZye1eUzAza6jhjmZJe5j6x1/A4sIqKsiINx+ZmeVquKYQEd0RsWyKrjsimjlqaU5595teA8BvvcV3EjUza6Q0G9jf8JpuAN64trvFlZiZzV2lCYUxvsyFmVljpQkFeVeCmVmu0oSCmZnly91h3OAopFeAfuCT6f4K84Y3H5mZNdbMUUSXk90E5+tkh6NuAo4BtgHXkV30bs7zVVLNzPI1s/norIj4UkTsSfdAuBo4JyK+CawsuL4Z5xUFM7PGmgmFqqT3Sqqk7r114+bNb6x3NJuZ5WsmFN4PfAB4IXUfAC6QtBj4aIG1mZnZLMvdp5B2JP/rBqPvnNlyilfgfXzMzOa93DUFSeslfVfSC5Kel3STpPWzUZyZmc2uZjYfXQ9sAV4LrAO+l4aZmdkC00wo9ETE9RExkrovAz0F11UYbzwyM2usmVDYKekCSW2puwDYVXRhM81HH5mZ5WsmFD4MvBf4FfAccD5wUZFFmZlZa+SGQkQ8HRHnRkRPRBwdEb8D/O4s1FYMbz8yM2vocC+I94kZrWIWyNuPzMxyHW4ozNtf2PCqgplZQ4cbCvPul3UsxX758mBL6zAzm8sahoKkPZJ2T9HtITtnYV4ZS7Erbn+0pXWYmc1lDS9zEREL6mbGvryFmVm+0tx5zZlgZpavsFCQdF26XtJDDcZL0hWStkt6QNIpRdUCUHUqmJnlKnJN4cvAWdOMPxs4IXUXA18ssBavKZiZNaGwUIiIHwMvTtPkPOCGyPwMWCFpbWH1FDVhM7MFpJX7FNYBz9Q935GGFcI7ms3M8rUyFKY6AW7KX25JF0vql9Q/MDBwWG/mSDAzy9fKUNgBbKh7vh54dqqGEXF1RPRFRF9Pz+FdtdsrCmZm+VoZCluAD6ajkE4HXomI54p7O6eCmVme3Hs0Hy5JNwJnAmsk7QD+FOgAiIirgFuBc4DtwH4Kvhy31xTMzPIVFgoRsTlnfAB/VNT7T1R1KJiZ5SrNGc1mZpavNKHgS2abmeUrTShUq62uwMxs7itNKBy7+igAVhzV0eJKzMzmrsJ2NM81SzvbWb1kEWe9+ZhWl2JmNmeVZk0BQPLZCmZm0ylVKIB8voKZ2TRKFQoSeF3BzKyxcoUCPrPZzGw65QoFORTMzKZTrlBAPonNzGwa5QoFrymYmU2rXKGAdzObmU2nVKEwOFJlcHi01WWYmc1ZpQqFF/cd4JYHCryPj5nZPFeqUDAzs+k5FMzMrMahYGZmNQ4FMzOrcSiYmVmNQ8HMzGocCmZmVlOaO68BHN+zhMEDPnnNzKyRUq0prF3eRaWiVpdhZjZnlWpN4f9t39XqEszM5rRSrSmYmdn0HApmZlbjUDAzs5pCQ0HSWZK2Sdou6VNTjP+QpAFJ96Xu94qsZ83SziInb2Y27xUWCpLagC8AZwMnApslnThF029GxEmpu6aoegDOP3U9i9q8cmRm1kiRv5CnAdsj4vGIOAB8AzivwPfLtXdomAOj1VaWYGY2pxUZCuuAZ+qe70jDJnqPpAckfVvShgLr4as/exqAZ17cX+TbmJnNW0WGwlRniU28RfL3gN6IeAvwf4C/nnJC0sWS+iX1DwwMHHFhviWnmdnUigyFHUD9f/7rgWfrG0TErogYSk//Cjh1qglFxNUR0RcRfT09PYdd0FvXLwfwWc1mZg0UGQp3AydI2ihpEbAJ2FLfQNLauqfnAo8UWA8f/o2NAAwNe7+CmdlUCguFiBgBPgrcRvZj/zcRsVXSZyWdm5pdKmmrpPuBS4EPFVUPwM0//yUA1975RJFvY2Y2bxV67aOIuBW4dcKwz9T1XwZcVmQN9f7gX7yOO7YN8LaNq2brLc3M5pVSHbTfu2YJAMNVbz4yM5tKqUKhuytbMdozONLiSszM5qZShcLijjbaK2L3q8OtLsXMbE4qVShIorurnd2DDgUzs6mUKhQAurs6vPnIzKyB0oXCyiWL2Ll3KL+hmVkJlS4Ujlt1FE/t8rWPzMymUrpQ6F19FM++/CoHRnxYqpnZRKULheNWL6EasOMlry2YmU1UwlA4CsCbkMzMplDCUMjOan5q174WV2JmNveULhTWLF3EkkVtPOk1BTOzSUoXCpI4bvUSrymYmU2hdKEA2X4F71MwM5uslKHwxrXLeGLXPl7Z78tdmJnVK2UonLZxFRFw95MvtroUM7M5pZShcNKGFSxqr3Dn9p2tLsXMbE4pZSh0dbRx5ht6uPXB5xitRqvLMTObM0oZCgDnnbSOF/YM8eNHB1pdipnZnFHaUHjniUezdnkX//uO7UR4bcHMDEocCp3tbfzhma/j7idf4m/ve7bV5ZiZzQmlDQWAf/e24+g7biX/+eaH2PrsK60ux8ys5UodCm0VccXmk+nuaueCa+7iJ4/5aCQzK7dShwLAa1cs5uu/fzqrl3ZywTV38envPsjzuwdbXZaZWUtovu1k7evri/7+/hmf7t6hEf7HD7Zxw0+fQsC733wM57x5Le94wxq6uzpm/P3MzGaTpHsioi+3nUNhvKd27eMrP32Km+7dwUv7h2mviDeuXcZJG1bwz9YtZ2PPEnpXL2HN0kVIKqwOM7OZ5FA4QiOjVe59+mX+ftsL/Pzpl3lgx8vsOzBaG9/d2c4xy7s4elknR3d30dPdSc/STpYv7mDZ4na6uzpY1tVBd1c7yxZ3sLSznY42OUjMrCWaDYX22ShmPmpvq3DaxlWctnEVAKPV4JkX9/PErn08uTPrfrV7kIE9Q9z95Iu8sGco977PFWVnUy/uaKOro43Ojgpd7W0sXtRGV+rv7KjQXqnQ3iY6xh7bKnS0ifa2Ch2V9Dg2rPZctFcqtFWElO1Eryjrsn6opGFtEpUK48fV+lNXIbU7+Jr66UogAIHInlckBGlc1mCs3cHXZI/Utasou6R57bUOTrOWKTQUJJ0F/E+gDbgmIv58wvhO4AbgVGAX8L6IeLLImg5XW0X0rllC75ol8GuTx0cEe4ZG2P3qMLtfHWH34DB7BrPnewaH2Ts0wuBwlVeHRxkcHmVwuMrgyCiDB0azx+EqL+0b5sBolZHRKsOjwUi1yshopGHZ8+HR+bVmdyQaBs2kcaoLqPR8LLRq0xofNOPHTXrnSXVMPWbya1XXYvK4ia9tHH6TXjuuhkOpr/F8T2ocUI0gSI9B6sYPy64ME1Trx1Wzx6Wd7XS2V6hURLUajFQjfXeDl/YfYOVRi1i+uD1NJ02DoFqFl/YfoL0iero7J9U99j4E4+tLrx1rMza9SO0iDvbvGRxm1ZJFtX2EEcFoNRhOf2MDe4ZYs3QRHW2VuvdlklTJOCOjwa59B9iwanHt9aNp3kerwWgEu/YOUQ04vmdJbT4mfrYAi9qyz2/SvAOb//mx/P47jp9c1AwqLBQktQFfAP4VsAO4W9KWiHi4rtlHgJci4vWSNgGfA95XVE1FksSytMmIlcW9T/0XeTiFxsholeFq9liN7MsYkX0Rs/5sWPZHGIxW0x9kNfsjGq31R2pHXX/qqvXtxv/hkb60Y3/o9X+QaXT2xxsH+8f+AMb+aHOnWTed+j+ohtMc95lN+Azrxk4eN/Hzbjx20mujvmVO2ybrm9h4cn0xzbjp3jMmjRsL2fFrbmNrgWmtrgLU1u7Gr+kNj2b/9IyMZt+99kq29jm2Njs0XGV4tMpoRFojrV+LFE/t2sfSrnaWLGrwszRprTPrZ1wdB9dGNaHdgZEq+w6MjPtc2tMa9qJ28fL+YZZ2tk8b9LVhEwbt2neAweFRepZ2MjRaRZDmv0JbBdoqFXbuHaJNoq1t/D8z9Z83wNBoNX3H4+B7p/k4elnn1J/NDCpyTeE0YHtEPA4g6RvAeUB9KJwH/Fnq/zZwpSTFfNvRMYskZV/kNlhMW6vLMbMFpsjzFNYBz9Q935GGTdkmIkaAV4DVBdZkZmbTKDIUptpgOnENoJk2SLpYUr+k/oEBX9XUzKwoRYbCDmBD3fP1wMQrz9XaSGoHlgOTbocWEVdHRF9E9PX09BRUrpmZFRkKdwMnSNooaRGwCdgyoc0W4MLUfz7wI+9PMDNrncJ2NEfEiKSPAreRHZJ6XURslfRZoD8itgDXAl+RtJ1sDWFTUfWYmVm+Qs9TiIhbgVsnDPtMXf8g8G+LrMHMzJpX+qukmpnZQQ4FMzOrmXcXxJM0ADx1mC9fA5TtTjqe53LwPJfDkczzcRGRe/jmvAuFIyGpv5mrBC4knudy8DyXw2zMszcfmZlZjUPBzMxqyhYKV7e6gBbwPJeD57kcCp/nUu1TMDOz6ZVtTcHMzKZRmlCQdJakbZK2S/pUq+s5XJI2SLpD0iOStkr6WBq+StIPJT2aHlem4ZJ0RZrvBySdUjetC1P7RyVd2Og95wpJbZJ+LumW9HyjpLtS/d9M19hCUmd6vj2N762bxmVp+DZJ727NnDRH0gpJ35b0i7S8z1joy1nSH6fv9UOSbpTUtdCWs6TrJL0g6aG6YTO2XCWdKunB9JorpEO8v21256uF3ZFde+kx4HhgEXA/cGKr6zrMeVkLnJL6u4F/Ak4E/gL4VBr+KeBzqf8c4Ptklyk/HbgrDV8FPJ4eV6b+la2ev5x5/wTwdeCW9PxvgE2p/yrgktT/h8BVqX8T8M3Uf2Ja9p3AxvSdaGv1fE0zv38N/F7qXwSsWMjLmez+Kk8Ai+uW74cW2nIG3gGcAjxUN2zGlivwj8AZ6TXfB84+pPpa/QHN0kI4A7it7vllwGWtrmuG5u1vyW55ug1Ym4atBbal/i8Bm+vab0vjNwNfqhs+rt1c68guvX478JvALekLvxNon7iMyS7CeEbqb0/tNHG517ebax2wLP1AasLwBbucOXjTrVVpud0CvHshLmegd0IozMhyTeN+UTd8XLtmurJsPmrmLnDzTlpdPhm4C3hNRDwHkB6PTs0azft8+0w+D/xHIN2mndXAy5HdsQ/G19/ojn7zaZ6PBwaA69Mms2skLWEBL+eI+CXw34GngefIlts9LOzlPGamluu61D9xeNPKEgpN3eFtPpG0FLgJ+HhE7J6u6RTDYprhc46k3wZeiIh76gdP0TRyxs2beSb7z/cU4IsRcTKwj2yzQiPzfp7TdvTzyDb5vBZYApw9RdOFtJzzHOo8HvG8lyUUmrkL3LwhqYMsEL4WEd9Jg5+XtDaNXwu8kIY3mvf59Jm8HThX0pPAN8g2IX0eWKHsjn0wvv5Gd/SbT/O8A9gREXel598mC4mFvJzfCTwREQMRMQx8B/h1FvZyHjNTy3VH6p84vGllCYVm7gI3L6QjCa4FHomIy+tG1d/F7kKyfQ1jwz+YjmI4HXglrZ7eBrxL0sr0H9q70rA5JyIui4j1EdFLtux+FBHvB+4gu2MfTJ7nqe7otwXYlI5a2QicQLZTbs6JiF8Bz0j6tTToXwIPs4CXM9lmo9MlHZW+52PzvGCXc50ZWa5p3B5Jp6fP8IN102pOq3e4zOKOnXPIjtR5DPiTVtdzBPPxG2Srgw8A96XuHLJtqbcDj6bHVam9gC+k+X4Q6Kub1oeB7am7qNXz1uT8n8nBo4+OJ/tj3w58C+hMw7vS8+1p/PF1r/+T9Fls4xCPymjBvJ4E9KdlfTPZUSYLejkD/wX4BfAQ8BWyI4gW1HIGbiTbZzJM9p/9R2ZyuQJ96fN7DLiSCQcr5HU+o9nMzGrKsvnIzMya4FAwM7Mah4KZmdU4FMzMrMahYGZmNQ4Fs0TSqKT76roZu5qupN76q2KazVXt+U3MSuPViDip1UWYtZLXFMxySHpS0uck/WPqXp+GHyfp9nSd+9slHZuGv0bSdyXdn7pfT5Nqk/RX6X4BP5C0OLW/VNLDaTrfaNFsmgEOBbN6iydsPnpf3bjdEXEa2Rmin0/DrgRuiIi3AF8DrkjDrwD+b0S8lex6RVvT8BOAL0TEm4CXgfek4Z8CTk7T+YOiZs6sGT6j2SyRtDcilk4x/EngNyPi8XQxwl9FxGpJO8mugT+chj8XEWskDQDrI2Kobhq9wA8j4oT0/D8BHRHxXyX9HbCX7FIWN0fE3oJn1awhrymYNSca9DdqM5Whuv5RDu7T+y2y69ucCtxTd0VQs1nnUDBrzvvqHn+a+n9CdtVWgPcDd6b+24FLoHZf6WWNJiqpAmyIiDvIbiK0Api0tmI2W/wfidlBiyXdV/f87yJi7LDUTkl3kf0jtTkNuxS4TtJ/ILtL2kVp+MeAqyV9hGyN4BKyq2JOpQ34qqTlZFfE/MuIeHnG5sjsEHmfglmOtE+hLyJ2troWs6J585GZmdV4TcHMzGq8pmBmZjUOBTMzq3EomJlZjUPBzMxqHApmZlbjUDAzs5r/Dy/wPleJQDKTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_val_ls)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Log Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039776649326086044"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See our final loss\n",
    "loss_val_ls[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that our model is doing the right thing, we do a quick test - create a new logistic regression model in PyTorch, but insert the weights learned from our scikit-learn model, then compute the loss similarly.\n",
    "\n",
    "Note that it is numerically similar to the loss above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039661891758441925\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    blank_model = LogisticRegressionPyTorch(n_in=x.shape[1], n_out=3)\n",
    "    blank_model.linear.weight.set_(\n",
    "        torch.from_numpy(sk_model.coef_).float()\n",
    "    )\n",
    "    blank_model.linear.bias.set_(\n",
    "        torch.from_numpy(sk_model.intercept_).float()\n",
    "    )\n",
    "    y_hat = blank_model(x)\n",
    "    print(criterion(y_hat, y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Exploration\n",
    "\n",
    "1. We asserted that the models are roughly equivalent because they reached the same losses. But is this true? Can we directly compare the parameter values? (Try it!) What if the parameter values are different?\n",
    "\n",
    "2. In scikit-learn, you can use `.predict_proba` to compute the predicted probabilities. How do we do the same for our PyTorch model?\n",
    "\n",
    "3. Although we showed that the loss is numerically the same, and you can confirm for yourself that the predictions $\\hat{y}$ are numerically similar between the scikit-learn and PyTorch implementations, if you inspect the actual weights and biases, you will notice that they are different. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the parameters are not similar. Similar loss level doen't gives similar models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0823e-08, 1.9161e-35],\n",
       "        [1.0000e+00, 6.7665e-07, 5.4951e-32],\n",
       "        [1.0000e+00, 1.3463e-07, 1.8380e-33],\n",
       "        [1.0000e+00, 1.8953e-06, 4.2526e-31],\n",
       "        [1.0000e+00, 7.2635e-09, 8.4490e-36],\n",
       "        [1.0000e+00, 1.4114e-08, 5.4430e-34],\n",
       "        [1.0000e+00, 2.1954e-07, 1.6169e-32],\n",
       "        [1.0000e+00, 6.7309e-08, 7.6260e-34],\n",
       "        [9.9999e-01, 6.3196e-06, 3.4291e-30],\n",
       "        [1.0000e+00, 3.4855e-07, 5.9948e-33],\n",
       "        [1.0000e+00, 2.3904e-09, 1.3676e-36],\n",
       "        [1.0000e+00, 2.8092e-07, 1.3384e-32],\n",
       "        [1.0000e+00, 4.2304e-07, 7.0605e-33],\n",
       "        [1.0000e+00, 1.6836e-07, 5.6920e-34],\n",
       "        [1.0000e+00, 7.3192e-12, 1.2484e-41],\n",
       "        [1.0000e+00, 3.2447e-11, 3.2323e-39],\n",
       "        [1.0000e+00, 5.3755e-10, 4.7664e-37],\n",
       "        [1.0000e+00, 2.3507e-08, 2.5911e-34],\n",
       "        [1.0000e+00, 5.2518e-09, 3.0235e-35],\n",
       "        [1.0000e+00, 6.4242e-09, 2.4620e-35],\n",
       "        [1.0000e+00, 1.0145e-07, 2.8274e-33],\n",
       "        [1.0000e+00, 2.8230e-08, 1.3119e-33],\n",
       "        [1.0000e+00, 9.4045e-10, 6.7436e-38],\n",
       "        [9.9999e-01, 5.2651e-06, 1.4453e-28],\n",
       "        [1.0000e+00, 3.2584e-06, 2.6291e-30],\n",
       "        [1.0000e+00, 2.5534e-06, 1.0687e-30],\n",
       "        [1.0000e+00, 7.1872e-07, 8.1070e-31],\n",
       "        [1.0000e+00, 1.8043e-08, 6.4105e-35],\n",
       "        [1.0000e+00, 1.6127e-08, 4.3453e-35],\n",
       "        [1.0000e+00, 1.5616e-06, 3.6107e-31],\n",
       "        [1.0000e+00, 2.3268e-06, 8.1883e-31],\n",
       "        [1.0000e+00, 9.3398e-08, 1.5301e-32],\n",
       "        [1.0000e+00, 1.2109e-10, 1.2664e-39],\n",
       "        [1.0000e+00, 2.2936e-11, 1.4253e-40],\n",
       "        [1.0000e+00, 3.4855e-07, 5.9948e-33],\n",
       "        [1.0000e+00, 2.3756e-08, 6.0275e-35],\n",
       "        [1.0000e+00, 1.4065e-09, 3.6163e-37],\n",
       "        [1.0000e+00, 3.4855e-07, 5.9948e-33],\n",
       "        [1.0000e+00, 1.3798e-06, 1.4970e-31],\n",
       "        [1.0000e+00, 4.9570e-08, 4.3890e-34],\n",
       "        [1.0000e+00, 1.4101e-08, 7.7447e-35],\n",
       "        [9.9969e-01, 3.0627e-04, 1.7180e-26],\n",
       "        [1.0000e+00, 3.3706e-07, 9.6416e-33],\n",
       "        [1.0000e+00, 1.6756e-06, 3.7624e-29],\n",
       "        [1.0000e+00, 3.6634e-07, 3.8019e-31],\n",
       "        [1.0000e+00, 1.9955e-06, 1.2912e-30],\n",
       "        [1.0000e+00, 6.6955e-09, 1.0583e-35],\n",
       "        [1.0000e+00, 4.1381e-07, 1.8565e-32],\n",
       "        [1.0000e+00, 3.2459e-09, 2.3762e-36],\n",
       "        [1.0000e+00, 6.0163e-08, 5.1692e-34],\n",
       "        [6.6223e-07, 9.9999e-01, 1.3483e-05],\n",
       "        [2.4925e-07, 9.9994e-01, 5.5883e-05],\n",
       "        [2.1630e-08, 9.9862e-01, 1.3779e-03],\n",
       "        [7.8365e-09, 9.9995e-01, 4.7842e-05],\n",
       "        [8.9077e-09, 9.9839e-01, 1.6108e-03],\n",
       "        [8.2428e-09, 9.9988e-01, 1.1648e-04],\n",
       "        [3.3325e-08, 9.9849e-01, 1.5056e-03],\n",
       "        [7.8930e-06, 9.9999e-01, 6.0657e-10],\n",
       "        [1.1563e-07, 9.9998e-01, 1.6700e-05],\n",
       "        [5.4674e-08, 9.9998e-01, 1.6896e-05],\n",
       "        [1.2481e-07, 1.0000e+00, 4.4971e-08],\n",
       "        [1.5299e-07, 9.9996e-01, 4.2936e-05],\n",
       "        [1.8318e-07, 1.0000e+00, 1.1252e-07],\n",
       "        [5.0904e-09, 9.9908e-01, 9.1530e-04],\n",
       "        [1.9168e-05, 9.9998e-01, 1.5756e-08],\n",
       "        [1.5164e-06, 1.0000e+00, 3.2487e-06],\n",
       "        [5.2608e-09, 9.9848e-01, 1.5214e-03],\n",
       "        [1.4882e-06, 1.0000e+00, 1.6888e-08],\n",
       "        [1.0970e-10, 9.3299e-01, 6.7009e-02],\n",
       "        [4.6516e-07, 1.0000e+00, 9.9020e-08],\n",
       "        [2.5474e-10, 5.6059e-01, 4.3941e-01],\n",
       "        [1.6656e-06, 1.0000e+00, 3.8901e-07],\n",
       "        [3.7857e-11, 7.5160e-01, 2.4840e-01],\n",
       "        [1.1878e-08, 9.9995e-01, 4.6021e-05],\n",
       "        [7.2740e-07, 1.0000e+00, 1.6146e-06],\n",
       "        [5.5194e-07, 9.9999e-01, 8.0958e-06],\n",
       "        [9.4602e-09, 9.9918e-01, 8.1496e-04],\n",
       "        [3.7811e-10, 6.9538e-01, 3.0462e-01],\n",
       "        [8.8427e-09, 9.9889e-01, 1.1056e-03],\n",
       "        [7.2870e-05, 9.9993e-01, 1.4671e-10],\n",
       "        [3.8326e-07, 1.0000e+00, 9.6089e-08],\n",
       "        [1.8843e-06, 1.0000e+00, 6.0094e-09],\n",
       "        [1.6166e-06, 1.0000e+00, 9.9275e-08],\n",
       "        [8.7096e-13, 1.1769e-01, 8.8231e-01],\n",
       "        [2.8505e-09, 9.9751e-01, 2.4888e-03],\n",
       "        [1.3817e-07, 9.9975e-01, 2.4592e-04],\n",
       "        [6.0173e-08, 9.9966e-01, 3.4246e-04],\n",
       "        [3.4484e-09, 9.9971e-01, 2.8940e-04],\n",
       "        [6.5255e-07, 1.0000e+00, 9.0367e-07],\n",
       "        [3.2082e-08, 9.9999e-01, 1.2614e-05],\n",
       "        [5.3694e-09, 9.9996e-01, 4.5245e-05],\n",
       "        [2.3331e-08, 9.9982e-01, 1.8315e-04],\n",
       "        [3.5296e-07, 1.0000e+00, 4.9651e-07],\n",
       "        [5.2970e-06, 1.0000e+00, 9.2317e-10],\n",
       "        [3.4803e-08, 9.9998e-01, 1.7141e-05],\n",
       "        [8.5017e-07, 1.0000e+00, 2.9127e-07],\n",
       "        [1.9347e-07, 1.0000e+00, 3.5319e-06],\n",
       "        [3.9451e-07, 1.0000e+00, 2.6438e-06],\n",
       "        [1.5722e-04, 9.9984e-01, 6.9918e-11],\n",
       "        [2.1644e-07, 1.0000e+00, 2.6785e-06],\n",
       "        [1.6906e-25, 2.2318e-10, 1.0000e+00],\n",
       "        [1.3217e-16, 3.3736e-04, 9.9966e-01],\n",
       "        [4.4626e-20, 8.3830e-07, 1.0000e+00],\n",
       "        [6.6213e-17, 2.4473e-04, 9.9975e-01],\n",
       "        [6.9716e-22, 7.8757e-08, 1.0000e+00],\n",
       "        [3.1505e-24, 3.9044e-09, 1.0000e+00],\n",
       "        [3.7589e-13, 9.7119e-02, 9.0288e-01],\n",
       "        [7.4044e-20, 3.9115e-06, 1.0000e+00],\n",
       "        [7.4095e-20, 6.9181e-06, 9.9999e-01],\n",
       "        [2.5683e-22, 5.9048e-09, 1.0000e+00],\n",
       "        [4.4031e-13, 8.4582e-03, 9.9154e-01],\n",
       "        [1.0764e-16, 2.2460e-04, 9.9977e-01],\n",
       "        [9.7149e-18, 1.7401e-05, 9.9998e-01],\n",
       "        [2.1067e-18, 2.8679e-05, 9.9997e-01],\n",
       "        [1.1520e-21, 7.0235e-08, 1.0000e+00],\n",
       "        [3.0582e-18, 4.1876e-06, 1.0000e+00],\n",
       "        [4.5708e-15, 2.0007e-03, 9.9800e-01],\n",
       "        [4.0470e-21, 6.4657e-08, 1.0000e+00],\n",
       "        [6.3206e-31, 5.2896e-13, 1.0000e+00],\n",
       "        [7.5910e-14, 7.0743e-02, 9.2926e-01],\n",
       "        [4.2412e-20, 3.3033e-07, 1.0000e+00],\n",
       "        [4.3105e-16, 4.2497e-04, 9.9957e-01],\n",
       "        [8.2014e-25, 3.1937e-09, 1.0000e+00],\n",
       "        [9.1345e-13, 4.5386e-02, 9.5461e-01],\n",
       "        [1.0123e-17, 1.5231e-05, 9.9998e-01],\n",
       "        [5.1197e-16, 3.8226e-04, 9.9962e-01],\n",
       "        [1.0638e-11, 1.5670e-01, 8.4330e-01],\n",
       "        [1.5967e-11, 1.7660e-01, 8.2340e-01],\n",
       "        [1.1809e-20, 6.6635e-07, 1.0000e+00],\n",
       "        [1.9872e-13, 2.5130e-02, 9.7487e-01],\n",
       "        [8.1589e-20, 2.7223e-06, 1.0000e+00],\n",
       "        [4.3889e-16, 6.9508e-05, 9.9993e-01],\n",
       "        [8.7327e-22, 1.0702e-07, 1.0000e+00],\n",
       "        [6.2869e-11, 7.7215e-01, 2.2785e-01],\n",
       "        [1.1637e-14, 2.9520e-02, 9.7048e-01],\n",
       "        [1.9871e-22, 1.4395e-08, 1.0000e+00],\n",
       "        [1.0287e-20, 1.1771e-07, 1.0000e+00],\n",
       "        [1.0355e-14, 3.0417e-03, 9.9696e-01],\n",
       "        [4.5355e-11, 3.0091e-01, 6.9909e-01],\n",
       "        [3.8662e-16, 1.1135e-04, 9.9989e-01],\n",
       "        [1.5326e-21, 4.2725e-08, 1.0000e+00],\n",
       "        [4.1534e-16, 4.8650e-05, 9.9995e-01],\n",
       "        [1.3217e-16, 3.3736e-04, 9.9966e-01],\n",
       "        [7.2232e-22, 3.9143e-08, 1.0000e+00],\n",
       "        [3.0270e-22, 1.0134e-08, 1.0000e+00],\n",
       "        [6.0062e-18, 5.9410e-06, 9.9999e-01],\n",
       "        [7.8334e-16, 7.8335e-04, 9.9922e-01],\n",
       "        [4.9156e-15, 8.7503e-04, 9.9912e-01],\n",
       "        [2.7056e-18, 3.7771e-06, 1.0000e+00],\n",
       "        [1.8637e-13, 1.9474e-02, 9.8053e-01]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(pt_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the multi-class regression is over-parameterized / not identifiable, which means there are more than one unique solution. Therefore, different models (like above sk_model and pt_model) can have similar loss with different weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
